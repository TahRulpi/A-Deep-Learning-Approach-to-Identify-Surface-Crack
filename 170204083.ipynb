{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n"
      ],
      "metadata": {
        "id": "14qoYXRpUvRQ",
        "execution": {
          "iopub.status.busy": "2021-12-24T20:36:46.93905Z",
          "iopub.execute_input": "2021-12-24T20:36:46.939946Z",
          "iopub.status.idle": "2021-12-24T20:36:48.090338Z",
          "shell.execute_reply.started": "2021-12-24T20:36:46.939839Z",
          "shell.execute_reply": "2021-12-24T20:36:48.089535Z"
        },
        "trusted": true
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Normal dataset**"
      ],
      "metadata": {
        "id": "8NPJcCVrFl9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-eLuPwHZPq6",
        "outputId": "b483c946-7141-407e-e415-3aacce792fae"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main_directory  = '/content/drive/MyDrive/Ass_1/train_Normal_128.npy'\n",
        "main_directory1  = '/content/drive/MyDrive/Ass_1/test_Normal_128.npy'\n",
        "train_Normal = np.load(main_directory)\n",
        "test_Normal = np.load(main_directory1)\n",
        "print(train_Normal)\n",
        "print(test_Normal)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ovDe35lZbxw",
        "outputId": "b428fff6-0acd-421f-9bc8-9786b89bf210"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[[ 53.]\n",
            "   [ 59.]\n",
            "   [ 67.]\n",
            "   ...\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   [  0.]]\n",
            "\n",
            "  [[ 50.]\n",
            "   [ 59.]\n",
            "   [ 70.]\n",
            "   ...\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   [  0.]]\n",
            "\n",
            "  [[ 47.]\n",
            "   [ 57.]\n",
            "   [ 68.]\n",
            "   ...\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   [  0.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[  0.]\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   ...\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   [  0.]]\n",
            "\n",
            "  [[  0.]\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   ...\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   [  0.]]\n",
            "\n",
            "  [[  0.]\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   ...\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   [  0.]]]\n",
            "\n",
            "\n",
            " [[[  0.]\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   ...\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   [  0.]]\n",
            "\n",
            "  [[  0.]\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   ...\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   [  0.]]\n",
            "\n",
            "  [[  0.]\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   ...\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   [  0.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[  0.]\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   ...\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   [  0.]]\n",
            "\n",
            "  [[  0.]\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   ...\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   [  0.]]\n",
            "\n",
            "  [[  0.]\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   ...\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   [  0.]]]\n",
            "\n",
            "\n",
            " [[[  4.]\n",
            "   [ 22.]\n",
            "   [ 34.]\n",
            "   ...\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   [  0.]]\n",
            "\n",
            "  [[  4.]\n",
            "   [ 18.]\n",
            "   [ 33.]\n",
            "   ...\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   [  0.]]\n",
            "\n",
            "  [[  1.]\n",
            "   [ 14.]\n",
            "   [ 26.]\n",
            "   ...\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   [  0.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[  0.]\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   ...\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   [  0.]]\n",
            "\n",
            "  [[  0.]\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   ...\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   [  0.]]\n",
            "\n",
            "  [[  0.]\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   ...\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   [  0.]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[  0.]\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   ...\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   [  0.]]\n",
            "\n",
            "  [[  0.]\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   ...\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   [  0.]]\n",
            "\n",
            "  [[  0.]\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   ...\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   [  0.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[  0.]\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   ...\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   [  0.]]\n",
            "\n",
            "  [[  0.]\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   ...\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   [  0.]]\n",
            "\n",
            "  [[  0.]\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   ...\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   [  0.]]]\n",
            "\n",
            "\n",
            " [[[ 29.]\n",
            "   [ 40.]\n",
            "   [ 50.]\n",
            "   ...\n",
            "   [116.]\n",
            "   [113.]\n",
            "   [120.]]\n",
            "\n",
            "  [[ 33.]\n",
            "   [ 39.]\n",
            "   [ 49.]\n",
            "   ...\n",
            "   [116.]\n",
            "   [119.]\n",
            "   [ 89.]]\n",
            "\n",
            "  [[ 45.]\n",
            "   [ 38.]\n",
            "   [ 49.]\n",
            "   ...\n",
            "   [112.]\n",
            "   [126.]\n",
            "   [ 78.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[  0.]\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   ...\n",
            "   [ 19.]\n",
            "   [  0.]\n",
            "   [  0.]]\n",
            "\n",
            "  [[  0.]\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   ...\n",
            "   [ 17.]\n",
            "   [  1.]\n",
            "   [  0.]]\n",
            "\n",
            "  [[  0.]\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   ...\n",
            "   [ 18.]\n",
            "   [  0.]\n",
            "   [  0.]]]\n",
            "\n",
            "\n",
            " [[[  9.]\n",
            "   [ 23.]\n",
            "   [ 37.]\n",
            "   ...\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   [  0.]]\n",
            "\n",
            "  [[  6.]\n",
            "   [ 19.]\n",
            "   [ 33.]\n",
            "   ...\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   [  0.]]\n",
            "\n",
            "  [[  2.]\n",
            "   [ 15.]\n",
            "   [ 25.]\n",
            "   ...\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   [  0.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[  0.]\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   ...\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   [  0.]]\n",
            "\n",
            "  [[  0.]\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   ...\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   [  0.]]\n",
            "\n",
            "  [[  0.]\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   ...\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   [  0.]]]]\n",
            "[[[[ 38.]\n",
            "   [ 33.]\n",
            "   [ 27.]\n",
            "   ...\n",
            "   [123.]\n",
            "   [116.]\n",
            "   [109.]]\n",
            "\n",
            "  [[ 38.]\n",
            "   [ 34.]\n",
            "   [ 27.]\n",
            "   ...\n",
            "   [124.]\n",
            "   [117.]\n",
            "   [105.]]\n",
            "\n",
            "  [[ 42.]\n",
            "   [ 31.]\n",
            "   [ 26.]\n",
            "   ...\n",
            "   [122.]\n",
            "   [113.]\n",
            "   [105.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 43.]\n",
            "   [ 36.]\n",
            "   [ 38.]\n",
            "   ...\n",
            "   [ 32.]\n",
            "   [ 35.]\n",
            "   [ 31.]]\n",
            "\n",
            "  [[ 42.]\n",
            "   [ 43.]\n",
            "   [ 41.]\n",
            "   ...\n",
            "   [ 29.]\n",
            "   [ 34.]\n",
            "   [ 35.]]\n",
            "\n",
            "  [[ 39.]\n",
            "   [ 33.]\n",
            "   [ 43.]\n",
            "   ...\n",
            "   [ 32.]\n",
            "   [ 33.]\n",
            "   [ 29.]]]\n",
            "\n",
            "\n",
            " [[[  8.]\n",
            "   [ 10.]\n",
            "   [ 11.]\n",
            "   ...\n",
            "   [ 14.]\n",
            "   [ 15.]\n",
            "   [ 13.]]\n",
            "\n",
            "  [[ 11.]\n",
            "   [  8.]\n",
            "   [  9.]\n",
            "   ...\n",
            "   [ 13.]\n",
            "   [ 14.]\n",
            "   [ 12.]]\n",
            "\n",
            "  [[  7.]\n",
            "   [  7.]\n",
            "   [  9.]\n",
            "   ...\n",
            "   [ 11.]\n",
            "   [ 12.]\n",
            "   [ 13.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 44.]\n",
            "   [ 19.]\n",
            "   [ 12.]\n",
            "   ...\n",
            "   [ 14.]\n",
            "   [ 12.]\n",
            "   [ 13.]]\n",
            "\n",
            "  [[ 42.]\n",
            "   [ 18.]\n",
            "   [  8.]\n",
            "   ...\n",
            "   [ 12.]\n",
            "   [ 11.]\n",
            "   [ 14.]]\n",
            "\n",
            "  [[ 43.]\n",
            "   [ 21.]\n",
            "   [ 13.]\n",
            "   ...\n",
            "   [ 13.]\n",
            "   [ 13.]\n",
            "   [ 12.]]]\n",
            "\n",
            "\n",
            " [[[ 19.]\n",
            "   [ 17.]\n",
            "   [ 18.]\n",
            "   ...\n",
            "   [ 19.]\n",
            "   [ 22.]\n",
            "   [ 22.]]\n",
            "\n",
            "  [[ 16.]\n",
            "   [ 17.]\n",
            "   [ 16.]\n",
            "   ...\n",
            "   [ 20.]\n",
            "   [ 21.]\n",
            "   [ 21.]]\n",
            "\n",
            "  [[ 15.]\n",
            "   [ 18.]\n",
            "   [ 18.]\n",
            "   ...\n",
            "   [ 20.]\n",
            "   [ 19.]\n",
            "   [ 22.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 33.]\n",
            "   [ 26.]\n",
            "   [ 23.]\n",
            "   ...\n",
            "   [ 17.]\n",
            "   [ 18.]\n",
            "   [ 19.]]\n",
            "\n",
            "  [[ 37.]\n",
            "   [ 27.]\n",
            "   [ 25.]\n",
            "   ...\n",
            "   [ 18.]\n",
            "   [ 21.]\n",
            "   [ 22.]]\n",
            "\n",
            "  [[ 36.]\n",
            "   [ 29.]\n",
            "   [ 28.]\n",
            "   ...\n",
            "   [ 27.]\n",
            "   [ 28.]\n",
            "   [ 29.]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[  0.]\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   ...\n",
            "   [  0.]\n",
            "   [  1.]\n",
            "   [  2.]]\n",
            "\n",
            "  [[  0.]\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   ...\n",
            "   [  6.]\n",
            "   [  7.]\n",
            "   [  9.]]\n",
            "\n",
            "  [[  0.]\n",
            "   [  0.]\n",
            "   [  1.]\n",
            "   ...\n",
            "   [  9.]\n",
            "   [ 11.]\n",
            "   [ 12.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[  0.]\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   ...\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   [  0.]]\n",
            "\n",
            "  [[  0.]\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   ...\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   [  0.]]\n",
            "\n",
            "  [[  0.]\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   ...\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   [  0.]]]\n",
            "\n",
            "\n",
            " [[[  0.]\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   ...\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   [  0.]]\n",
            "\n",
            "  [[  0.]\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   ...\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   [  0.]]\n",
            "\n",
            "  [[  0.]\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   ...\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   [  0.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[  0.]\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   ...\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   [  0.]]\n",
            "\n",
            "  [[  0.]\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   ...\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   [  0.]]\n",
            "\n",
            "  [[  0.]\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   ...\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   [  0.]]]\n",
            "\n",
            "\n",
            " [[[  0.]\n",
            "   [  1.]\n",
            "   [ 13.]\n",
            "   ...\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   [  0.]]\n",
            "\n",
            "  [[  0.]\n",
            "   [  0.]\n",
            "   [ 11.]\n",
            "   ...\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   [  0.]]\n",
            "\n",
            "  [[  0.]\n",
            "   [  0.]\n",
            "   [ 12.]\n",
            "   ...\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   [  0.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[  0.]\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   ...\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   [  0.]]\n",
            "\n",
            "  [[  0.]\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   ...\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   [  0.]]\n",
            "\n",
            "  [[  0.]\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   ...\n",
            "   [  0.]\n",
            "   [  0.]\n",
            "   [  0.]]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import tensor\n",
        "traindata = [train_Normal[i] for i in range(len(train_Normal))]\n",
        "#train = torch.stack([d[0] for d in traindata], dim=0)\n",
        "train =torch.from_numpy(np.array(traindata))\n",
        "train=train[0:59999]        \n",
        "ys = [d[1] for d in traindata]\n",
        "train_y = tensor(ys)\n",
        "\n",
        "testdata = [test_Normal[i] for i in range(len(test_Normal))]\n",
        "#test = torch.stack([d[0] for d in testdata], dim=0)  \n",
        "#test =torch.from_numpy(np.array(testdata))     \n",
        "Ntest=np.vstack(testdata).astype(np.float64)\n",
        "test =torch.from_numpy(Ntest)\n",
        "test=test[0:9999]\n",
        "ys = [d[1] for d in testdata]\n",
        "Ny=np.vstack(ys).astype(np.float64)\n",
        "test_y = tensor(Ny)"
      ],
      "metadata": {
        "id": "v_cY8zB9ZgEp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70496495-f06d-4b52-9fab-9e8c0f7f3d7d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  import sys\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "x=int(torch.rand(1,)*1000)\n",
        "train = torch.rand(6000, 128, 128, 1)          \n",
        "test = torch.rand(6000, 128, 128, 1) \n",
        "train_y = torch.randint(83,x, (1000,)) \n",
        "print(train_y)\n",
        "test_y = torch.randint(83,x, (1000,))\n",
        "print(test_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpVSC__PoJ-_",
        "outputId": "9546ecca-7fb2-4476-f978-e5db10902a77"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([758, 267, 762, 243, 845, 616, 788, 589, 682, 453, 831, 355, 486, 671,\n",
            "        616, 290, 491, 396, 350, 686, 676, 660, 221, 177, 108, 509, 788, 821,\n",
            "        146, 691, 371, 736, 245, 632, 580, 546, 106, 281, 183, 258, 684, 659,\n",
            "        120, 510, 382, 796, 161, 741, 224, 579, 793, 220, 613, 552, 882, 841,\n",
            "        818, 136, 268, 412, 618, 541, 620, 772, 894, 188, 455, 653, 459, 551,\n",
            "        883, 442, 146, 426, 541, 533, 282, 309, 826, 326, 646, 397, 231,  85,\n",
            "        406, 385, 333, 226, 681, 323, 174, 425, 639, 196, 209, 101, 841, 666,\n",
            "        880, 778, 788, 805, 336, 883, 739, 857, 716, 761, 556, 505, 716, 843,\n",
            "        310, 716, 434, 415, 138, 376, 410, 854, 514, 498, 149, 853, 822, 786,\n",
            "        107, 558, 114, 792, 200, 840, 750, 904, 684, 470, 303, 164, 891, 332,\n",
            "        853, 101, 220, 378, 728, 813, 534, 638, 524, 749, 696, 334, 347, 119,\n",
            "        606, 758, 121, 201, 733, 114, 583, 442, 489, 370, 385, 267, 739, 579,\n",
            "        753, 518, 724, 487,  97, 838, 380, 330, 819, 383, 675, 132, 476, 452,\n",
            "        752, 584, 769, 470, 852,  88, 648, 299, 763, 694, 142, 763, 802, 601,\n",
            "        212, 602, 185, 530, 320, 275, 475, 309, 614, 540, 901, 714, 668, 664,\n",
            "        362, 840, 241, 129, 479, 533, 262, 174, 803, 229, 767, 225, 828, 667,\n",
            "        636, 698, 454, 691, 336, 243, 773, 745, 723, 321, 823, 848, 279, 579,\n",
            "        496, 294, 263, 341, 249, 889, 419, 631, 171, 804, 210, 105, 812, 408,\n",
            "        636, 624, 884, 446, 562, 596, 122, 372, 802, 302, 167, 737, 588, 650,\n",
            "        612, 729, 597, 579, 686, 160, 662, 548, 540, 690, 211,  84, 196, 190,\n",
            "        488, 559, 320, 436, 758, 856, 842, 466, 531, 595, 199, 371,  91, 787,\n",
            "        406, 108, 695, 143, 644, 677, 182, 676, 288, 281, 887,  96, 440, 443,\n",
            "        176, 548, 534, 309, 561, 504, 768, 160, 559, 184, 342, 165, 708, 555,\n",
            "        804, 417, 252, 293, 101, 401, 877, 839, 692, 776, 629, 691, 397, 136,\n",
            "        306, 123, 659, 609, 619, 158, 189, 512, 371, 665, 477, 731, 735, 113,\n",
            "        592, 101, 660, 640, 865, 503, 777, 406, 746, 732, 774, 208, 426, 840,\n",
            "        689, 134, 292, 176, 581, 552, 903, 209, 661, 570, 900, 698, 321, 579,\n",
            "        246, 792, 693, 652, 131, 286, 575, 763, 252, 170, 304, 792,  84, 408,\n",
            "        730, 601, 579, 398,  85, 683, 180, 267, 150, 500,  91, 758, 203, 208,\n",
            "        356, 490, 333, 500, 903, 588, 193, 283, 476, 678, 895, 175, 502, 280,\n",
            "        827, 176,  87, 327, 672, 662, 246, 776, 150, 590, 265, 177, 780, 894,\n",
            "        265, 405, 433, 610, 603, 200, 827, 494, 675, 846, 855, 554, 654, 799,\n",
            "        317, 185, 790, 341, 852, 308, 342, 904, 315, 782, 831, 136, 160, 481,\n",
            "        641, 721, 200, 262, 238, 449, 244, 795, 893, 767, 295, 374, 317, 848,\n",
            "        469, 261, 821, 363, 752, 577, 668, 900, 757, 306, 480, 583, 611, 617,\n",
            "        213, 806, 783, 752, 609, 483, 807,  91, 561, 655, 875, 654, 773, 616,\n",
            "        132, 420, 484, 703, 287, 756, 136, 313, 702, 422, 832, 695, 500, 272,\n",
            "        182, 476, 295, 654, 786, 178, 103, 816, 728, 801, 398, 834,  93, 120,\n",
            "        740, 595, 850, 177, 863, 684, 400, 579, 155, 379, 409, 610, 428, 214,\n",
            "        707, 775, 106, 853, 576, 160, 691, 702, 636, 833, 897, 475, 390, 699,\n",
            "        146, 746, 361, 771, 534, 757, 335, 830, 277, 239, 711, 770, 518, 887,\n",
            "        135, 539, 701, 906, 429, 515, 177, 894, 395, 275, 125, 441, 890, 548,\n",
            "        281, 143, 189, 270, 107, 130, 710, 365, 589, 289, 573, 647, 833, 608,\n",
            "        725, 489, 370, 390, 419, 292, 153, 748, 543, 814,  88, 659, 532, 805,\n",
            "        588, 516, 282, 346, 609, 102, 829, 462, 249, 527, 724, 364, 721, 225,\n",
            "        767, 187, 227, 671, 686, 289, 866, 589, 653, 643, 381, 348, 644, 402,\n",
            "        884, 110, 535, 282, 699, 458, 346, 668, 340, 709, 674, 226, 235, 144,\n",
            "         93, 519, 408, 396, 699, 444, 531, 745, 329, 485, 167, 362, 751, 734,\n",
            "        324, 723, 487, 194, 162, 342, 628, 733, 865, 444, 491, 716, 343, 585,\n",
            "        585, 177, 562, 300, 858, 335, 345, 140, 255, 400,  94, 269, 175, 811,\n",
            "        239, 136, 231, 501, 802, 610, 809, 378, 302, 584, 103, 485, 394, 519,\n",
            "        623, 371, 723, 742, 581, 262, 540, 587, 575, 209, 730, 424, 820, 719,\n",
            "        855, 327, 565, 370, 691, 213, 146, 180, 633, 650, 583, 180, 514, 437,\n",
            "         89, 426, 687, 690, 791, 634, 835, 866, 611, 708, 142, 319, 425, 744,\n",
            "        843, 776, 753, 380, 709, 176, 518, 601, 537, 515, 876, 107, 813, 158,\n",
            "        773, 223, 193, 878, 499, 389, 180, 225, 339, 619, 128, 111, 273, 559,\n",
            "        396, 806, 432, 327,  87, 743, 104, 888, 293, 285, 403, 438, 825, 529,\n",
            "        367, 586, 618, 300, 877, 242, 465, 174, 527, 442, 253, 669, 570, 704,\n",
            "        227, 464, 409, 314, 469, 618, 400,  97, 350, 759, 800, 675, 149, 687,\n",
            "        260, 265, 640, 625, 254, 345, 240, 461, 348, 247, 537, 453, 659, 673,\n",
            "        549, 201, 473, 362, 480, 634, 583, 118, 551, 687, 850, 502, 366, 189,\n",
            "        408, 449, 635, 522, 622, 688, 205, 162, 799, 577, 427, 621, 161, 434,\n",
            "        391, 123, 242, 723, 413, 478, 424, 795, 442, 228, 302, 254, 866, 694,\n",
            "        854, 855, 829, 373,  83, 490, 245, 475, 328, 729, 473, 258, 346, 556,\n",
            "        542, 427, 728, 523, 842, 195, 644, 101, 569, 231, 668, 533, 887, 367,\n",
            "        235, 236, 813, 504, 314, 280, 337, 617, 341, 389, 639, 394, 832, 874,\n",
            "        427, 413, 526, 252, 381, 658, 246, 416, 125, 546, 439, 508, 682, 240,\n",
            "        334, 102, 534, 461, 463, 723, 490, 377, 333, 377, 172, 497, 837, 697,\n",
            "        763, 287, 147, 347, 519, 499, 108, 328, 810, 397, 489, 595, 609, 298,\n",
            "        236, 562, 766, 848, 359, 855, 190, 755, 334, 435, 509, 220, 390, 270,\n",
            "        513, 276, 535, 900, 485, 755, 199, 654, 515, 293, 742,  86, 341, 438,\n",
            "        672, 466, 791, 399, 846, 364])\n",
            "tensor([604, 533, 711, 646, 544, 748, 648, 680, 207, 558, 148, 903, 567, 504,\n",
            "        632, 396, 599, 437, 238, 696, 852, 547, 723, 323, 128, 178, 652, 191,\n",
            "        415, 906, 258, 869, 772, 796, 350, 337, 215, 552, 619, 865, 619, 436,\n",
            "        200, 283, 172, 633, 564, 331, 622, 294, 902, 658, 394, 363, 440,  95,\n",
            "        502, 636, 415, 329, 727, 393, 421, 822, 504, 900, 637, 787, 867, 847,\n",
            "        684,  93, 733,  87, 831, 277, 663, 820, 295, 465, 807, 839, 646, 458,\n",
            "        487, 545, 516, 296, 214, 173, 344,  91, 135, 378, 510, 620, 553, 247,\n",
            "        858, 528, 528, 896, 522, 722, 395, 760, 853, 706, 163, 428, 825, 393,\n",
            "        331, 871, 561, 232, 445, 261, 534, 594, 127, 640, 474, 451, 108, 254,\n",
            "        576, 697, 150, 364, 679, 181, 120, 781, 684, 242, 154, 290,  86, 241,\n",
            "        280, 849, 792, 746, 629, 568, 633, 289, 644, 182, 168, 683, 448, 337,\n",
            "        596, 500, 797, 787, 150, 296, 688, 568, 825, 538, 851, 306, 549, 776,\n",
            "        202, 319, 536, 494, 832, 756, 260, 414, 680, 796, 134,  99, 799, 546,\n",
            "        657, 298, 492, 723, 851, 562, 731, 331, 726, 122, 278, 506, 877, 252,\n",
            "        556, 319, 626, 650, 166, 225, 560, 200, 405, 653, 280, 131, 277, 311,\n",
            "        478, 102, 366, 399, 806, 129, 689, 634, 136, 704, 473, 240, 196, 650,\n",
            "        800, 432, 537, 628, 730, 421, 736, 613, 119, 889, 733, 123, 263, 194,\n",
            "        340, 678, 219, 397, 790, 862, 692, 180,  90, 827,  87, 568, 739, 189,\n",
            "        532, 541, 267, 660, 641, 709, 278, 565, 480, 331, 801, 152, 635, 805,\n",
            "        623, 166, 600, 574, 129, 336, 224, 320, 618, 664, 221, 751, 884, 108,\n",
            "        257, 873, 772, 894, 735, 745, 316, 163, 607, 188, 593, 251, 838, 769,\n",
            "        631, 807, 102, 753, 852, 516, 606, 767, 196,  86, 440, 512, 388, 661,\n",
            "        562, 517, 411, 616, 741, 477, 517, 785, 413, 167, 101, 374, 151, 674,\n",
            "        845, 682, 721, 508, 388, 198, 235, 130, 674, 816, 367, 155, 804, 869,\n",
            "        652, 367, 204, 634, 363,  95, 767, 635, 202, 666, 295, 431, 419, 263,\n",
            "        260, 724, 298, 564, 420, 122, 330, 130, 456, 596, 700, 563, 192, 102,\n",
            "        105,  93, 417,  89, 843, 543, 795, 570, 287, 348, 424, 319, 865, 535,\n",
            "        819, 519, 805, 461, 737, 167, 155, 255, 560, 741, 703, 607, 431, 295,\n",
            "        138, 776, 627, 789, 351, 400, 785, 623, 434,  99, 816, 468, 133, 629,\n",
            "        699, 555, 565, 433, 432,  92, 194,  98, 820, 468, 111, 134, 386, 653,\n",
            "        755, 823, 534, 680, 380, 664, 555, 813, 618,  91, 860, 481, 823, 888,\n",
            "        282, 639, 220, 159, 341,  86, 280, 645, 824, 681, 498, 688, 113, 886,\n",
            "        385, 661, 615, 609, 585, 786, 812, 405, 326, 576, 194, 351, 532, 246,\n",
            "        294, 537, 227, 702, 296, 813,  91, 548, 547, 783, 146, 119, 512, 676,\n",
            "        172, 692, 716, 615, 143, 138, 400, 319, 825, 352, 722, 798, 802, 677,\n",
            "        505, 736, 334, 587, 774, 554, 139, 330,  93, 805,  93, 328, 461, 762,\n",
            "        506, 319, 387, 407, 334, 132, 830, 339, 119, 792, 135, 338, 534, 180,\n",
            "         89, 439, 612, 461, 731, 416, 691, 189, 288, 600, 830, 392, 694, 611,\n",
            "        393, 817, 729, 785, 635, 172, 282, 627, 635, 301, 498, 271, 331, 150,\n",
            "        886, 703, 383, 379, 407, 723,  98, 293, 354, 227, 145, 179, 723, 868,\n",
            "        253, 881, 441, 883, 213, 244, 236, 131, 489, 492, 792, 205, 662, 523,\n",
            "        659, 549, 668, 375, 383, 476, 528, 524, 131, 756, 762, 807, 858, 202,\n",
            "        431, 431, 423, 153, 185, 322, 307, 285, 693, 385, 418, 598, 238, 279,\n",
            "        817, 754, 876, 714, 490, 413, 686, 384, 523, 845, 764, 598, 475, 428,\n",
            "        548, 313, 522, 836, 705, 903, 663, 239, 683, 539, 380, 790, 507, 891,\n",
            "        400, 256, 596, 197, 274, 510, 483,  93, 830, 607, 301, 639, 712, 481,\n",
            "        847, 286, 262, 287,  99, 265, 247, 545, 269, 892, 773, 230, 266, 592,\n",
            "        783, 102, 158, 138, 587, 367, 535, 687, 447, 598, 461, 575, 213, 313,\n",
            "        572, 484, 152, 659, 636, 556, 302, 244, 570, 400, 303, 754, 793, 165,\n",
            "        377, 378, 591, 601, 874, 862, 199, 117, 626, 788, 616, 441, 446, 660,\n",
            "        293, 197, 652, 235, 547, 155, 348, 472, 509,  98, 306, 205, 494, 803,\n",
            "        857, 212, 831, 350, 828, 633, 680, 753, 262, 314, 134, 795, 762, 893,\n",
            "        360, 514, 371, 499, 439, 324, 699, 503, 677, 393, 704, 326, 515, 497,\n",
            "        127, 748, 154, 577, 207, 773, 872, 366, 654, 119, 123, 625, 195, 208,\n",
            "        272, 718, 458, 151, 305, 193, 141, 845, 678, 158, 210, 472, 625, 460,\n",
            "        749, 336, 469, 297, 359, 503, 154, 709, 639, 713, 751, 223, 655, 210,\n",
            "        480, 869, 600, 538, 762, 719, 618, 371, 234, 320, 885, 503, 866, 217,\n",
            "        774, 542, 128, 643, 604, 553, 713, 656, 264, 281, 101, 554, 856, 573,\n",
            "        513, 627, 393, 313, 904, 535, 658, 759, 578, 829, 591,  96, 692, 532,\n",
            "        324, 414, 499, 485, 185, 705, 406, 745,  87, 659, 269, 896, 216, 242,\n",
            "        535, 607, 832, 245, 892,  92, 873, 163, 599, 239, 677, 651, 544, 792,\n",
            "        107, 736, 486, 400, 621, 364, 363, 107, 585, 723, 208, 559, 664, 727,\n",
            "        323, 412, 155, 905, 414, 542, 821, 564, 725, 794, 656, 869, 120, 785,\n",
            "        214, 507, 565, 173, 629, 494, 821, 792, 673, 284, 710,  95, 506, 752,\n",
            "        467, 777, 150, 768, 647, 880, 732, 763, 545, 735, 134, 729, 515, 274,\n",
            "        234, 131, 149, 759, 628, 800, 304, 326, 118, 565, 169, 347, 742, 871,\n",
            "        795, 646, 682, 704, 204, 449, 880, 266, 548, 570, 287,  96, 320, 217,\n",
            "        616, 545, 621, 526, 632, 329, 758, 771, 244, 133, 129, 563, 868, 278,\n",
            "        423, 647, 707, 786, 530, 163, 533, 578, 467, 677, 307, 124, 569, 449,\n",
            "        616, 280, 572,  92, 262, 879, 249, 443, 300, 901, 669, 795, 128, 303,\n",
            "        456, 103, 197, 557, 866, 456, 616, 744, 116, 289, 374, 313, 691, 792,\n",
            "        586, 194, 773, 535, 165, 696])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CNN**"
      ],
      "metadata": {
        "id": "Asqel7WlImno"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Model Architeture*"
      ],
      "metadata": {
        "id": "hunT82EVIvAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.cnn_layer_1 = nn.Conv2d(in_channels=1, out_channels=16,kernel_size=5, stride=1, padding=2)\n",
        "        self.cnn_layer_2 = nn.Conv2d(in_channels=16, out_channels=32,kernel_size=5, stride=1, padding=2)\n",
        "        \n",
        "        self.flatten = nn.Flatten()\n",
        "        self.maxpool = nn.MaxPool2d(2,2)\n",
        "        \n",
        "        self.linear_layer_1 = nn.Linear(32*32*32, 900) \n",
        "        self.linear_layer_2 = nn.Linear(900, 768)\n",
        "        self.linear_layer_3 = nn.Linear(768, 512)   \n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.dropout = nn.Dropout(.2)\n",
        "        \n",
        "        # self.flatten = nn.Flatten()\n",
        "        \n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.cnn_layer_1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        \n",
        "        #print(x.shape)\n",
        "        \n",
        "        x = self.cnn_layer_2(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        \n",
        "        #print(x.shape)\n",
        "        \n",
        "        x = self.flatten(x)\n",
        "        #print(x.shape)\n",
        "        \n",
        "        x = self.linear_layer_1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(x)\n",
        "        \n",
        "        x = self.linear_layer_2(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(x)\n",
        "        \n",
        "        x = self.linear_layer_3(x)\n",
        "        #logits = self.sigmoid(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "zP2YrygD470L"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNetwork(784)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wufTzQ3ECPdv",
        "outputId": "71a23257-6c5d-463e-a4eb-a970334f705d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (cnn_layer_1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "  (cnn_layer_2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (linear_layer_1): Linear(in_features=32768, out_features=900, bias=True)\n",
            "  (linear_layer_2): Linear(in_features=900, out_features=768, bias=True)\n",
            "  (linear_layer_3): Linear(in_features=768, out_features=512, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (sigmoid): Sigmoid()\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "summary(model,(1,128,128))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7wdIeDLbHpv",
        "outputId": "56794e28-f0e1-45ff-8595-9a21a2cd0fc5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 16, 128, 128]             416\n",
            "           Dropout-2         [-1, 16, 128, 128]               0\n",
            "              ReLU-3         [-1, 16, 128, 128]               0\n",
            "         MaxPool2d-4           [-1, 16, 64, 64]               0\n",
            "            Conv2d-5           [-1, 32, 64, 64]          12,832\n",
            "           Dropout-6           [-1, 32, 64, 64]               0\n",
            "              ReLU-7           [-1, 32, 64, 64]               0\n",
            "         MaxPool2d-8           [-1, 32, 32, 32]               0\n",
            "           Flatten-9                [-1, 32768]               0\n",
            "           Linear-10                  [-1, 900]      29,492,100\n",
            "          Dropout-11                  [-1, 900]               0\n",
            "             ReLU-12                  [-1, 900]               0\n",
            "           Linear-13                  [-1, 768]         691,968\n",
            "          Dropout-14                  [-1, 768]               0\n",
            "             ReLU-15                  [-1, 768]               0\n",
            "           Linear-16                  [-1, 512]         393,728\n",
            "================================================================\n",
            "Total params: 30,591,044\n",
            "Trainable params: 30,591,044\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.06\n",
            "Forward/backward pass size (MB): 10.04\n",
            "Params size (MB): 116.70\n",
            "Estimated Total Size (MB): 126.80\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Optimizer*"
      ],
      "metadata": {
        "id": "0hmu-8_NbTNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=.001)"
      ],
      "metadata": {
        "id": "uq1EWSj6bUvQ"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}